{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq   # Or ChatOpenAI if you prefer OpenAI\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_core.output_parsers import StrOutputParser\n",
    "#from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith Tracking\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Gen AI-Krish Naik\\5. Personal Gen AI Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set embedding tech\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and fetch the index path from faiss db\n",
    "#PDF_PATH = \"Query_Data.pdf\"\n",
    "#INDEX_PATH = \"faiss_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import FAISS\n",
    "# load from disk\n",
    "#new_db = FAISS.load_local(INDEX_PATH,embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk -chromadb\n",
    "from langchain_chroma import Chroma\n",
    "new_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='697d5e50-22cc-44d9-92ba-be58eae49886', metadata={}, page_content=\"Q: How do I reset my password?\\nA: Click on 'Forgot Password' on the login page and follow the steps.\\nQ: What should I do if my laptop doesn’t turn on?\\nA: Please check the power connection and try holding the power button for 10 seconds.\\nQ: My payment failed but money deducted.\\nA: Please provide your transaction ID. We will check and process a refund if applicable.\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Do you support Facebook Messenger?\"\n",
    "docs=new_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever object to query similar documents\n",
    "retriever = new_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2. Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "## load the GROQ API Key\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Template\n",
    "## Prompt Template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Document chain for creation-custom prompt for answering questions with context.\n",
    "\n",
    "document_chain =  create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Can you integrate with Salesforce?\",\n",
    "    \"context\":[Document(page_content=\"Q: How do I reset my password?\\nA: Click on 'Forgot Password' on the login page and follow the steps.\\nQ: What should I do if my laptop doesn’t turn on?\\nA: Please check the power connection and try holding the power button for 10 seconds.\\nQ: My payment failed but money deducted.\\nA: Please provide your transaction ID. We will check and process a refund if applicable.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x224c4f17bf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Conversational Retriever Setup\n",
    "retriever = new_db.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "Rag_chains = create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=Rag_chains.invoke({\"input\":\"Who manages payroll queries?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User and AI Chat History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Prompt template for conversation\n",
    "\n",
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. \"\n",
    "    \"Rewrite the user question into a standalone query using chat history if needed.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000224A6149C60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Rewrite the user question into a standalone query using chat history if needed.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# History-aware retriever (rephrases user queries based on conversation)\n",
    "history_aware_retriever = create_history_aware_retriever( llm, retriever, retriever_prompt )\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000224A6149C60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Rewrite the user question into a standalone query using chat history if needed.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000224C4F17BF0>, search_kwargs={})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000224A6149C60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8501430>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C873C830>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full retrieval chain (retriever + doc_chain)\n",
    "new_document_chain = create_stuff_documents_chain(llm,qa_prompt)\n",
    "new_rag_retriever_chain = create_retrieval_chain(history_aware_retriever,new_document_chain)\n",
    "new_rag_retriever_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chat conversation history store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context does not mention WhatsApp.\n"
     ]
    }
   ],
   "source": [
    "# manual chat history store\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "chat_history=[]\n",
    "question=\"Can I chat on WhatsApp?\"\n",
    "response1=new_rag_retriever_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response1[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "question2=\"How do I update my billing information?\"\n",
    "response2=new_rag_retriever_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "print(response2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can I chat on WhatsApp?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't know. The context does not mention WhatsApp.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically store all chat history \n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap your RAG chain with history support\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    new_rag_retriever_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversational_rag_chain.invoke(\n",
    "        {\"input\": \"How do I update my billing information? \"},\n",
    "        config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How do I update my billing information? ',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(id='697d5e50-22cc-44d9-92ba-be58eae49886', metadata={}, page_content=\"Q: How do I reset my password?\\nA: Click on 'Forgot Password' on the login page and follow the steps.\\nQ: What should I do if my laptop doesn’t turn on?\\nA: Please check the power connection and try holding the power button for 10 seconds.\\nQ: My payment failed but money deducted.\\nA: Please provide your transaction ID. We will check and process a refund if applicable.\")],\n",
       " 'answer': \"I don't know. The given context does not provide information on updating billing information.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_history(session_id: str, question: str):\n",
    "    \"\"\"\n",
    "    Ask a question with multi-user chat history support.\n",
    "    Each session_id has its own stored conversation.\n",
    "    \"\"\"\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: Unfortunately, I don't have the information on how to update billing information in the given context.\n",
      "User2: I don't know.\n",
      "\n",
      "User1 Chat History:\n",
      "HUMAN: How do I update my billing information? \n",
      "AI: I don't know. The given context does not provide information on updating billing information.\n",
      "HUMAN: How do I update my billing information? \n",
      "AI: Unfortunately, I don't have the information on how to update billing information in the given context.\n"
     ]
    }
   ],
   "source": [
    "# User 1 asks questions\n",
    "print(\"User1:\", ask_with_history(\"user1\", \"How do I update my billing information? \"))\n",
    "\n",
    "# User 2 starts a different session\n",
    "print(\"User2:\", ask_with_history(\"user2\", \"My product needs an update\"))\n",
    "\n",
    "# Check history for user1\n",
    "history1 = get_session_history(\"user1\")\n",
    "print(\"\\nUser1 Chat History:\")\n",
    "for msg in history1.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: Please check the power connection and try holding the power button for 10 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"User1:\", ask_with_history(\"user1\", \"What should I do if my laptop doesn’t turn on?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: If holding the power button for 10 seconds doesn't work, you should try checking if the power cord is securely plugged in and if the outlet is working properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"User1:\", ask_with_history(\"user1\", \"What next\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: I didn't provide any previous information to continue from. If you'd like to ask another question or provide more context, I'd be happy to help.\n"
     ]
    }
   ],
   "source": [
    "print(\"User1:\", ask_with_history(\"user1\", \"What next\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Can I chat on WhatsApp?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know. The context does not mention WhatsApp.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk -chromadb\n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000224C8FED040>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000224C8FECDA0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "llm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Rewrite the user’s question into a clear, standalone query.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a polite and user-friendly support assistant.\\n\"\n",
    "     \"Use the following KB context to answer the user question clearly:\\n\\n\"\n",
    "     \"Context:\\n{context}\\n\\n\"\n",
    "     \"Guidelines:\\n\"\n",
    "     \"- Always explain in simple and helpful language.\\n\"\n",
    "     \"- If the KB does not have enough info, say: 'I’m not sure about that. Please contact support.'\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "history_aware_retriever = create_history_aware_retriever(llm2, retriever, retriever_prompt)\n",
    "doc_chain = create_stuff_documents_chain(llm2, qa_prompt)\n",
    "retrieval_chain = create_retrieval_chain(history_aware_retriever, doc_chain)\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    retrieval_chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Bot: I'm happy to help you with that. Unfortunately, updating billing information isn't something that's covered in our current knowledge base. \n",
      "\n",
      "I’m not sure about that. Please contact support. They'll be able to guide you through the process and ensure that your billing information is updated correctly.\n",
      "\n",
      "\n",
      "🤖 Bot: If you'd like to contact support, you can try a few options. \n",
      "\n",
      "Firstly, you can check our website for a contact page or a support email address where you can send a message.\n",
      "\n",
      "Alternatively, if you're looking for immediate assistance, you can try our live chat feature (if it's available) or give us a call (if we have a phone number listed).\n",
      "\n",
      "If none of these options are available, you can try reaching out to our support team via our social media channels.\n",
      "\n",
      "Remember, our support team is here to help you, and they'll be able to assist you with updating your billing information.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7. Demo Chat\n",
    "# =========================\n",
    "def ask_bot(user_id, query):\n",
    "    result = conversational_chain.invoke(\n",
    "        {\"input\": query, \"context\": \"\"},\n",
    "        config={\"configurable\": {\"session_id\": user_id}}\n",
    "    )\n",
    "    return result[\"answer\"]\n",
    "\n",
    "print(\"🤖 Bot:\", ask_bot(\"user123\", \"How do I update my billing information?\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"🤖 Bot:\", ask_bot(\"user123\", \"what next?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
