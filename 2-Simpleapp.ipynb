{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq   # Or ChatOpenAI if you prefer OpenAI\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_core.output_parsers import StrOutputParser\n",
    "#from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith Tracking\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Gen AI-Krish Naik\\5. Personal Gen AI Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set embedding tech\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and fetch the index path\n",
    "PDF_PATH = \"Customer.pdf\"\n",
    "INDEX_PATH = \"faiss_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# load from disk\n",
    "new_db = FAISS.load_local(INDEX_PATH,embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Customer.pdf', 'page': 0}, page_content='ticket_id  \\nchannel  \\ncustomer_query  intent  priority  \\nagent_response  \\n1001  \\nSocial  \\nLaptop  wonâ€™t  start  even  after  charging.  \\ntier1_triage  \\nHigh  \\nGuide  troubleshooting,  create  ticket,  escalate  to  L2.  \\n1002  \\nSocial  \\nThe  app  keeps  crashing  when  I  try  to  log  in.  \\ntechnical_issue  \\nHigh  \\nGather  logs,  create  ticket,  escalate  to  technical  team.  \\n1003  \\nWhatsApp  \\nMy  router  light  is  blinking  red.  tier1_triage  \\nMedium  \\nGuide  through  restart  steps,  escalate  if  unresolved.  \\n1004  \\nChatbot  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nHigh  \\nConfirm  order  status,  update  address,  send  confirmation.  \\n1005  \\nPhone  \\nMy  payment  was  deducted  twice,  what  should  I  do?  \\nrefund_request  \\nMedium  \\nAcknowledge,  verify,  initiate  refund,  confirm  turnaround  time.  \\n1006  \\nSocial  \\nWe  noticed  you  havenâ€™t  logged  in  recently,  need  help?  \\nproactive_outreach  \\nLow'),\n",
       " Document(metadata={'source': 'Customer.pdf', 'page': 4}, page_content='1047  \\nProactive  \\nIâ€™m  interested  in  switching  from  competitor,  whatâ€™s  the  process?  \\npresales_inquiry  \\nMedium  \\nExplain  onboarding  steps,  offer  demo,  connect  with  sales.  \\n1048  \\nPhone  \\nI  ordered  a  phone  last  week,  but  I  havenâ€™t  received  tracking  details.  \\norder_status  \\nMedium  \\nApologize,  check  system,  provide  tracking  ID,  escalate  if  delayed.  \\n1049  \\nProactive  \\nDo  you  have  any  discount  running  on  headphones?  \\nlead_qualification  \\nHigh  \\nInform  offers,  confirm  interest,  forward  lead  to  sales.  \\n1050  \\nWhatsApp  \\nLaptop  wonâ€™t  start  even  after  charging.  \\ntier1_triage  \\nLow  \\nGuide  troubleshooting,  create  ticket,  escalate  to  L2.  \\n1051  \\nEmail  \\nPlease  share  the  updated  leave  policy  document.  \\nknowledge_lookup  \\nMedium  \\nRetrieve  latest  HR  policy  and  attach.  \\n1052  \\nPhone  \\nWhere  is  my  recent  order?  Still  not  shipped.  \\norder_status  \\nLow'),\n",
       " Document(metadata={'source': 'Customer.pdf', 'page': 4}, page_content='knowledge_lookup  \\nMedium  \\nRetrieve  latest  HR  policy  and  attach.  \\n1052  \\nPhone  \\nWhere  is  my  recent  order?  Still  not  shipped.  \\norder_status  \\nLow  \\nCheck  shipment  status,  provide  update,  escalate  if  pending.  \\n1053  \\nEmail  \\nIâ€™m  interested  in  switching  from  competitor,  whatâ€™s  the  process?  \\npresales_inquiry  \\nLow  \\nExplain  onboarding  steps,  offer  demo,  connect  with  sales.  \\n1054  \\nProactive  \\nLaptop  wonâ€™t  start  even  after  charging.  \\ntier1_triage  \\nLow  \\nGuide  troubleshooting,  create  ticket,  escalate  to  L2.  \\n1055  \\nEmail  \\nWe  noticed  you  havenâ€™t  logged  in  recently,  need  help?  \\nproactive_outreach  \\nMedium  \\nAsk  if  assistance  needed,  share  support  options.  \\n1056  \\nPhone  \\nPlease  share  the  updated  leave  policy  document.  \\nknowledge_lookup  \\nLow  \\nRetrieve  latest  HR  policy  and  attach.  \\n1057  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nLow'),\n",
       " Document(metadata={'source': 'Customer.pdf', 'page': 7}, page_content='1087  \\nPhone  \\nWhere  is  my  recent  order?  Still  not  shipped.  \\norder_status  \\nLow  \\nCheck  shipment  status,  provide  update,  escalate  if  pending.  \\n1088  \\nChatbot  \\nAre  there  EMI  options  available  on  laptops?  \\nlead_qualification  \\nHigh  \\nExplain  EMI  plans,  confirm  interest,  escalate  to  sales.  \\n1089  \\nPhone  \\nMy  payment  was  deducted  twice,  what  should  I  do?  \\nrefund_request  \\nHigh  \\nAcknowledge,  verify,  initiate  refund,  confirm  turnaround  time.  \\n1090  \\nPhone  \\nIâ€™m  interested  in  switching  from  competitor,  whatâ€™s  the  process?  \\npresales_inquiry  \\nMedium  \\nExplain  onboarding  steps,  offer  demo,  connect  with  sales.  \\n1091  \\nChatbot  \\nI  want  to  cancel  my  order  before  it  ships.  \\norder_modification  \\nLow  \\nCheck  order  status,  cancel  if  possible,  confirm  cancellation.  \\n1092  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nHigh')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"customer_query  intent  priority\"\n",
    "docs=new_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id  \n",
      "channel  \n",
      "customer_query  intent  priority  \n",
      "agent_response  \n",
      "1001  \n",
      "Social  \n",
      "Laptop  wonâ€™t  start  even  after  charging.  \n",
      "tier1_triage  \n",
      "High  \n",
      "Guide  troubleshooting,  create  ticket,  escalate  to  L2.  \n",
      "1002  \n",
      "Social  \n",
      "The  app  keeps  crashing  when  I  try  to  log  in.  \n",
      "technical_issue  \n",
      "High  \n",
      "Gather  logs,  create  ticket,  escalate  to  technical  team.  \n",
      "1003  \n",
      "WhatsApp  \n",
      "My  router  light  is  blinking  red.  tier1_triage  \n",
      "Medium  \n",
      "Guide  through  restart  steps,  escalate  if  unresolved.  \n",
      "1004  \n",
      "Chatbot  \n",
      "Can  I  change  my  delivery  address  before  shipping?  \n",
      "order_modification  \n",
      "High  \n",
      "Confirm  order  status,  update  address,  send  confirmation.  \n",
      "1005  \n",
      "Phone  \n",
      "My  payment  was  deducted  twice,  what  should  I  do?  \n",
      "refund_request  \n",
      "Medium  \n",
      "Acknowledge,  verify,  initiate  refund,  confirm  turnaround  time.  \n",
      "1006  \n",
      "Social  \n",
      "We  noticed  you  havenâ€™t  logged  in  recently,  need  help?  \n",
      "proactive_outreach  \n",
      "Low\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever object to query similar documents\n",
    "retriever = new_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2. Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "## load the GROQ API Key\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Document chain for creation-custom prompt for answering questions with context.\n",
    "\n",
    "document_chains =  create_stuff_documents_chain(llm,prompt)\n",
    "document_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The app keeps crashing when you try to log in. I would suggest gathering logs and creating a ticket to escalate to the technical team. This will help them diagnose and resolve the issue.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chains.invoke({\n",
    "    \"input\":\"The  app  keeps  crashing  when  I  try  to  log  in.\",\n",
    "    \"context\":[Document(page_content=\"The  app  keeps  crashing  when  I  try  to  log  in. technical_issue High Gather  logs,  create  ticket,  escalate  to  technical  team. 1003 WhatsApp  My  router  light  is  blinking  red.  tier1_triage. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1c43624a900>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "new_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Conversational Retriever Setup\n",
    "retriever = new_db.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "Rag_chains = create_retrieval_chain(retriever,document_chains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gather logs, create ticket, escalate to technical team.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=Rag_chains.invoke({\"input\":\"The  app  keeps  crashing  when  I  try  to  log  in.\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User and AI Chat History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Prompt template for conversation\n",
    "qa_user_prompts = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the context to answer the question.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001C416AE9BC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Use the context to answer the question.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# History-aware retriever (rephrases user queries based on conversation)\n",
    "history_aware_retriever = create_history_aware_retriever( llm, retriever, qa_user_prompts )\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001C416AE9BC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Use the context to answer the question.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C43624A900>, search_kwargs={})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001C416AE9BC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C4364FB8C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C425E2EDB0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full retrieval chain (retriever + doc_chain)\n",
    "new_document_chain = create_stuff_documents_chain(llm,qa_prompt)\n",
    "new_rag_chain = create_retrieval_chain(history_aware_retriever,new_document_chain)\n",
    "new_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chat conversation history store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see you've asked the same question before. Check shipment status, provide update, escalate if pending.\n"
     ]
    }
   ],
   "source": [
    "# manual chat history store\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "chat_history=[]\n",
    "question=\"Where is my recent order?\"\n",
    "response1=new_rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response1[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "question2=\"why My router light is blinking red.\"\n",
    "response2=new_rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "print(response2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Where is my recent order?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Check shipment status, provide update, escalate if pending.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically store all chat history \n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap your RAG chain with history support\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    new_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conversational_rag_chain.invoke(\n",
    "        {\"input\": \"Can I change my delivery  address before shipping?\"},\n",
    "        config={\"configurable\": {\"session_id\": \"user1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Can I change my delivery  address before shipping?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'Customer.pdf', 'page': 7}, page_content='Low  \\nCheck  order  status,  cancel  if  possible,  confirm  cancellation.  \\n1092  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nHigh  \\nConfirm  order  status,  update  address,  send  confirmation.'),\n",
       "  Document(metadata={'source': 'Customer.pdf', 'page': 4}, page_content='knowledge_lookup  \\nLow  \\nRetrieve  latest  HR  policy  and  attach.  \\n1057  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nLow  \\nConfirm  order  status,  update  address,  send  confirmation.'),\n",
       "  Document(metadata={'source': 'Customer.pdf', 'page': 8}, page_content='1098  \\nEmail  \\nWe  noticed  you  havenâ€™t  logged  in  recently,  need  help?  \\nproactive_outreach  \\nLow  \\nAsk  if  assistance  needed,  share  support  options.  \\n1099  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nMedium  \\nConfirm  order  status,  update  address,  send  confirmation.  \\n1100  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nMedium  \\nConfirm  order  status,  update  address,  send  confirmation.'),\n",
       "  Document(metadata={'source': 'Customer.pdf', 'page': 7}, page_content='1087  \\nPhone  \\nWhere  is  my  recent  order?  Still  not  shipped.  \\norder_status  \\nLow  \\nCheck  shipment  status,  provide  update,  escalate  if  pending.  \\n1088  \\nChatbot  \\nAre  there  EMI  options  available  on  laptops?  \\nlead_qualification  \\nHigh  \\nExplain  EMI  plans,  confirm  interest,  escalate  to  sales.  \\n1089  \\nPhone  \\nMy  payment  was  deducted  twice,  what  should  I  do?  \\nrefund_request  \\nHigh  \\nAcknowledge,  verify,  initiate  refund,  confirm  turnaround  time.  \\n1090  \\nPhone  \\nIâ€™m  interested  in  switching  from  competitor,  whatâ€™s  the  process?  \\npresales_inquiry  \\nMedium  \\nExplain  onboarding  steps,  offer  demo,  connect  with  sales.  \\n1091  \\nChatbot  \\nI  want  to  cancel  my  order  before  it  ships.  \\norder_modification  \\nLow  \\nCheck  order  status,  cancel  if  possible,  confirm  cancellation.  \\n1092  \\nWhatsApp  \\nCan  I  change  my  delivery  address  before  shipping?  \\norder_modification  \\nHigh')],\n",
       " 'answer': 'You can change your delivery address before shipping. The process involves checking your order status, updating the address, and sending a confirmation, which can be done through WhatsApp or other channels. The priority of this task is High.'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_history(session_id: str, question: str):\n",
    "    \"\"\"\n",
    "    Ask a question with multi-user chat history support.\n",
    "    Each session_id has its own stored conversation.\n",
    "    \"\"\"\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: I've checked the shipment status, and it's still pending. I'll provide an update, and if it's delayed, I'll escalate the issue.\n",
      "User1: Yes, we have an ongoing offer on headphones. I can inform you about the details and confirm your interest, and then forward you to our sales team for further assistance.\n",
      "User2: EMI options are available on laptops. Explain EMI plans, confirm interest, and escalate to sales.\n",
      "\n",
      "User1 Chat History:\n",
      "HUMAN: Where is my recent order? Still not shipped?\n",
      "AI: I've checked the shipment status, and it's still pending. I'll provide an update, and if it's delayed, I'll escalate the issue.\n",
      "HUMAN: Do you have any discount running on headphones? \n",
      "AI: Yes, we have an ongoing offer on headphones. I can inform you about the details and confirm your interest, and then forward you to our sales team for further assistance.\n"
     ]
    }
   ],
   "source": [
    "# User 1 asks questions\n",
    "print(\"User1:\", ask_with_history(\"user1\", \"Where is my recent order? Still not shipped?\"))\n",
    "print(\"User1:\", ask_with_history(\"user1\", \"Do you have any discount running on headphones? \"))\n",
    "\n",
    "# User 2 starts a different session\n",
    "print(\"User2:\", ask_with_history(\"user2\", \"Are there EMI options available on laptops?\"))\n",
    "\n",
    "# Check history for user1\n",
    "history1 = get_session_history(\"user1\")\n",
    "print(\"\\nUser1 Chat History:\")\n",
    "for msg in history1.messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
